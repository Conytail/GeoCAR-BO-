{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üß¨ BioFoundry Active Learning with Geometric Deep Learning\n",
    "\n",
    "**Corrected & Production-Ready Version**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook implements the complete DBTL (Design-Build-Test-Learn) cycle for CAR-T engineering:\n",
    "\n",
    "1. **Geometric Feature Learning**: Train EquiformerV2 on AlphaFold structures\n",
    "2. **Embedding Extraction**: Use corrected Hook method (not direct model output)\n",
    "3. **Active Learning**: Batch Diversity Sampling (pool-based approximation)\n",
    "4. **Iterative Optimization**: Manual validation + model update loop\n",
    "\n",
    "### Key Corrections Applied:\n",
    "- ‚úÖ Embedding extraction via `register_forward_hook`\n",
    "- ‚úÖ Renamed MOBO-OSD ‚Üí Batch Diversity Sampling (academic honesty)\n",
    "- ‚úÖ GPU-adaptive configurations (T4/V100/A100)\n",
    "- ‚úÖ Production-grade dependency installation order\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Based on correcting.md analysis  \n",
    "**Runtime**: 2-6 hours (depends on GPU: T4 ~6h, V100 ~3h, A100 ~2h)  \n",
    "**Prerequisites**: LMDB datasets uploaded to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell1-header"
   },
   "source": [
    "## üîß Cell 1: Environment Check & GPU Verification\n",
    "\n",
    "First, verify GPU access and auto-configure based on GPU type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell1-code"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information:\")\n",
    "print(\"=\" * 60)\n",
    "subprocess.run([\"nvidia-smi\"], check=False)\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Auto-configure based on GPU type\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if \"A100\" in gpu_name:\n",
    "        RECOMMENDED_BATCH_SIZE = 16\n",
    "        RECOMMENDED_LMAX = [4]\n",
    "    elif \"V100\" in gpu_name:\n",
    "        RECOMMENDED_BATCH_SIZE = 8\n",
    "        RECOMMENDED_LMAX = [4]\n",
    "    elif \"T4\" in gpu_name:\n",
    "        RECOMMENDED_BATCH_SIZE = 4\n",
    "        RECOMMENDED_LMAX = [2]  # Critical: T4 cannot handle lmax=4\n",
    "    else:\n",
    "        RECOMMENDED_BATCH_SIZE = 4\n",
    "        RECOMMENDED_LMAX = [2]\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Recommended Config for {gpu_name}:\")\n",
    "    print(f\"  - batch_size: {RECOMMENDED_BATCH_SIZE}\")\n",
    "    print(f\"  - lmax_list: {RECOMMENDED_LMAX}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected!\")\n",
    "    RECOMMENDED_BATCH_SIZE = 1\n",
    "    RECOMMENDED_LMAX = [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell2-header"
   },
   "source": [
    "## üì¶ Cell 2: Install Dependencies (Corrected Order)\n",
    "\n",
    "‚ö†Ô∏è **Critical**: Follow this exact installation order to avoid version conflicts.\n",
    "\n",
    "This implements the production-grade sequence from `correcting.md`:\n",
    "1. Uninstall existing PyG components\n",
    "2. Install specific PyTorch version\n",
    "3. Install PyG with matching CUDA version\n",
    "4. Install scipy 1.13.1 for `sph_harm` compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell2-code"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Installing Dependencies...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Uninstall existing PyG (avoid conflicts)\n",
    "!pip uninstall -y torch-scatter torch-sparse torch-geometric torch-cluster\n",
    "\n",
    "# Step 2: Install PyTorch (stable version for Colab)\n",
    "!pip install torch==2.1.0 torchvision==0.16.0\n",
    "\n",
    "# Step 3: Install PyG with CUDA 12.1 (Colab default)\n",
    "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv \\\n",
    "    -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "\n",
    "!pip install torch-geometric\n",
    "\n",
    "# Step 4: Install other dependencies\n",
    "!pip install lmdb pyyaml tqdm biopython ase e3nn timm \\\n",
    "    scipy==1.13.1 \\\n",
    "    numba wandb tensorboard \\\n",
    "    scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell3-header"
   },
   "source": [
    "## üìÇ Cell 3: Mount Google Drive & Upload Data\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL MODIFICATION REQUIRED**:\n",
    "\n",
    "Change `DRIVE_DATA_PATH` to your actual Google Drive path!\n",
    "\n",
    "```python\n",
    "DRIVE_DATA_PATH = \"/content/drive/My Drive/BioFoundry/data\"  # ‚Üê MODIFY THIS\n",
    "```\n",
    "\n",
    "**Why copy to local disk?**\n",
    "- LMDB read from Google Drive is 10-100√ó slower\n",
    "- This step is MANDATORY for acceptable training speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell3-code"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è MODIFY THIS PATH ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "DRIVE_DATA_PATH = \"/content/drive/My Drive/BioFoundry/data\"  # ‚Üê Change to your path\n",
    "\n",
    "LOCAL_DATA_PATH = \"/content/data\"\n",
    "CHECKPOINT_PATH = \"/content/checkpoints\"\n",
    "EMBEDDING_PATH = \"/content/embeddings.npy\"\n",
    "\n",
    "# Create local directories\n",
    "os.makedirs(LOCAL_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# Copy LMDB from Drive to local disk\n",
    "print(\"Copying LMDB files from Google Drive to local disk...\")\n",
    "print(\"‚è≥ This may take 2-5 minutes...\")\n",
    "\n",
    "if os.path.exists(DRIVE_DATA_PATH):\n",
    "    shutil.copytree(DRIVE_DATA_PATH, LOCAL_DATA_PATH, dirs_exist_ok=True)\n",
    "    print(f\"‚úÖ Data copied to {LOCAL_DATA_PATH}\")\n",
    "    \n",
    "    # Verify files\n",
    "    print(\"\\nData directory contents:\")\n",
    "    !ls -lh {LOCAL_DATA_PATH}\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: {DRIVE_DATA_PATH} not found!\")\n",
    "    print(\"Please upload train.lmdb and val.lmdb to Google Drive first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell4-header"
   },
   "source": [
    "## üì• Cell 4: Clone Code Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell4-code"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content\")\n",
    "\n",
    "# Clone OCP (Open Catalyst Project)\n",
    "if not os.path.exists(\"/content/ocp\"):\n",
    "    !git clone https://github.com/Open-Catalyst-Project/ocp.git\n",
    "    print(\"‚úÖ OCP cloned\")\n",
    "\n",
    "# Clone EquiformerV2\n",
    "if not os.path.exists(\"/content/equiformer_v2\"):\n",
    "    !git clone https://github.com/atomicarchitects/equiformer_v2.git\n",
    "    print(\"‚úÖ EquiformerV2 cloned\")\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, \"/content/ocp\")\n",
    "sys.path.insert(0, \"/content/equiformer_v2\")\n",
    "\n",
    "print(\"\\n‚úÖ Code repositories ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell5-header"
   },
   "source": [
    "## ‚öôÔ∏è Cell 5: Generate Training Configuration (GPU-Adaptive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell5-code"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config = {\n",
    "    \"trainer\": \"energy_v2\",\n",
    "    \"dataset\": {\n",
    "        \"train\": {\n",
    "            \"src\": f\"{LOCAL_DATA_PATH}/train.lmdb\",\n",
    "            \"normalize_labels\": False\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"src\": f\"{LOCAL_DATA_PATH}/val.lmdb\"\n",
    "        }\n",
    "    },\n",
    "    \"logger\": \"tensorboard\",\n",
    "    \"task\": {\n",
    "        \"dataset\": \"lmdb_v2\",\n",
    "        \"description\": \"BioFoundry Active Learning - Geometric Features\",\n",
    "        \"type\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"primary_metric\": \"mae\",\n",
    "        \"labels\": [\"predicted_score\"]\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": \"equiformer_v2\",\n",
    "        \"use_pbc\": False,\n",
    "        \"regress_forces\": False,\n",
    "        \"otf_graph\": True,\n",
    "        \"max_neighbors\": 20,\n",
    "        \"max_radius\": 12.0,\n",
    "        \"max_num_elements\": 90,\n",
    "        \"num_layers\": 4,\n",
    "        \"sphere_channels\": 64,\n",
    "        \"attn_hidden_channels\": 64,\n",
    "        \"num_heads\": 4,\n",
    "        \"attn_alpha_channels\": 64,\n",
    "        \"attn_value_channels\": 32,\n",
    "        \"ffn_hidden_channels\": 128,\n",
    "        \"norm_type\": \"layer_norm\",\n",
    "        \"lmax_list\": RECOMMENDED_LMAX,\n",
    "        \"mmax_list\": [2] if RECOMMENDED_LMAX == [4] else [1],\n",
    "        \"grid_resolution\": 18 if RECOMMENDED_LMAX == [4] else 8\n",
    "    },\n",
    "    \"optim\": {\n",
    "        \"batch_size\": RECOMMENDED_BATCH_SIZE,\n",
    "        \"eval_batch_size\": RECOMMENDED_BATCH_SIZE * 2,\n",
    "        \"num_workers\": 2,\n",
    "        \"lr_initial\": 0.001,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"optimizer_params\": {\"weight_decay\": 0.01},\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"scheduler_params\": {\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": 5,\n",
    "            \"epochs\": 50\n",
    "        },\n",
    "        \"mode\": \"min\",\n",
    "        \"max_epochs\": 50,\n",
    "        \"energy_coefficient\": 1.0,\n",
    "        \"eval_every\": 5,\n",
    "        \"checkpoint_every\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = \"/content/colab_config.yml\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úÖ Configuration saved to {config_path}\")\n",
    "print(f\"\\nBatch size: {RECOMMENDED_BATCH_SIZE}\")\n",
    "print(f\"Lmax: {RECOMMENDED_LMAX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell6-header"
   },
   "source": [
    "## üöÄ Cell 6: Train EquiformerV2\n",
    "\n",
    "‚è∞ **Expected Runtime**: 2-6 hours (GPU dependent)\n",
    "\n",
    "Monitor progress with TensorBoard (Cell 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell6-code"
   },
   "outputs": [],
   "source": [
    "os.environ['PYTHONPATH'] = '/content/ocp:/content/equiformer_v2'\n",
    "os.chdir(\"/content/equiformer_v2\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting EquiformerV2 Training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python main_oc20.py \\\n",
    "    --config-yml {config_path} \\\n",
    "    --mode train \\\n",
    "    --run-dir {CHECKPOINT_PATH} \\\n",
    "    --print-every 10\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Checkpoints: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell7-header"
   },
   "source": [
    "## üìä Cell 7: TensorBoard Monitoring (Optional)\n",
    "\n",
    "Run this in a separate tab while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell7-code"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {CHECKPOINT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell8-header"
   },
   "source": [
    "## üß¨ Cell 8: Extract Geometric Embeddings (CORRECTED)\n",
    "\n",
    "### ‚ö†Ô∏è Key Correction from `correcting.md`:\n",
    "\n",
    "EquiformerV2's `forward()` only returns energy (scalar), **NOT** embeddings!\n",
    "\n",
    "We use `register_forward_hook` to capture intermediate features **before** the energy head.\n",
    "\n",
    "### üîß May Require Modification:\n",
    "\n",
    "```python\n",
    "hook_layer_name = 'energy_block'  # ‚Üê Verify this matches your model\n",
    "```\n",
    "\n",
    "If error occurs, check the printed model structure and update the layer name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell8-code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import lmdb\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Extracting Geometric Embeddings...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Load checkpoint\n",
    "checkpoint_files = [f for f in os.listdir(CHECKPOINT_PATH) if f.endswith('.pt')]\n",
    "best_checkpoint = sorted(checkpoint_files)[-1]\n",
    "checkpoint_path = os.path.join(CHECKPOINT_PATH, best_checkpoint)\n",
    "\n",
    "print(f\"Loading: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "\n",
    "# 2. Reconstruct model\n",
    "from ocpmodels.common.registry import registry\n",
    "\n",
    "config_dict = checkpoint.get('config', config)\n",
    "model = registry.get_model_class(config_dict['model']['name'])(\n",
    "    **config_dict['model']\n",
    ")\n",
    "\n",
    "state_dict = checkpoint['state_dict']\n",
    "state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model loaded\")\n",
    "\n",
    "# 3. Print model structure to find correct layer\n",
    "print(\"\\nModel structure (first 20 layers):\")\n",
    "for i, (name, module) in enumerate(model.named_modules()):\n",
    "    print(f\"  {name}: {type(module).__name__}\")\n",
    "    if i > 20:\n",
    "        print(\"  ...\")\n",
    "        break\n",
    "\n",
    "# 4. Define Hook\n",
    "features_cache = {}\n",
    "\n",
    "def get_embedding_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        features_cache[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# 5. Register hook (‚ö†Ô∏è May need to modify layer name)\n",
    "hook_layer_name = 'energy_block'\n",
    "\n",
    "if hasattr(model, hook_layer_name):\n",
    "    hook_handle = getattr(model, hook_layer_name).register_forward_pre_hook(\n",
    "        lambda m, inp: features_cache.update({'embedding': inp[0].detach()})\n",
    "    )\n",
    "    print(f\"‚úÖ Hook registered at: {hook_layer_name}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Layer '{hook_layer_name}' not found. Using fallback...\")\n",
    "    hook_handle = model.norm.register_forward_hook(get_embedding_hook('embedding'))\n",
    "    print(\"‚úÖ Hook registered at: model.norm (fallback)\")\n",
    "\n",
    "# 6. Create DataLoader\n",
    "class LMDBDataset:\n",
    "    def __init__(self, lmdb_path):\n",
    "        self.env = lmdb.open(lmdb_path, readonly=True, lock=False)\n",
    "        with self.env.begin() as txn:\n",
    "            self.length = txn.stat()['entries']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with self.env.begin() as txn:\n",
    "            data = pickle.loads(txn.get(str(idx).encode()))\n",
    "        return Data(**data)\n",
    "\n",
    "train_dataset = LMDBDataset(f\"{LOCAL_DATA_PATH}/train.lmdb\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataset: {len(train_dataset)} samples\")\n",
    "\n",
    "# 7. Extract embeddings\n",
    "embeddings_dict = {}\n",
    "\n",
    "print(\"\\nExtracting embeddings...\")\n",
    "for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "    batch = batch.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(batch)\n",
    "        \n",
    "        emb = features_cache['embedding']\n",
    "        \n",
    "        # Aggregate to graph-level if needed\n",
    "        if emb.dim() == 2:\n",
    "            from torch_geometric.nn import global_mean_pool\n",
    "            emb = global_mean_pool(emb, batch.batch)\n",
    "        \n",
    "        emb_np = emb.cpu().numpy()\n",
    "        \n",
    "        # Store with sample IDs\n",
    "        sample_ids = batch.sid if hasattr(batch, 'sid') else \\\n",
    "                     [f\"train_{batch_idx * 16 + i}\" for i in range(len(emb_np))]\n",
    "        \n",
    "        for sid, embedding in zip(sample_ids, emb_np):\n",
    "            embeddings_dict[str(sid)] = embedding\n",
    "\n",
    "np.save(EMBEDDING_PATH, embeddings_dict)\n",
    "hook_handle.remove()\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted {len(embeddings_dict)} embeddings\")\n",
    "print(f\"‚úÖ Saved to {EMBEDDING_PATH}\")\n",
    "\n",
    "# Sample output\n",
    "sample_key = list(embeddings_dict.keys())[0]\n",
    "sample_emb = embeddings_dict[sample_key]\n",
    "print(f\"\\nSample shape: {sample_emb.shape}\")\n",
    "print(f\"Sample dims (first 5): {sample_emb[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell9-header"
   },
   "source": [
    "## üéØ Cell 9: Active Learning - Batch Diversity Sampling\n",
    "\n",
    "### ‚ö†Ô∏è Important Note (from `correcting.md`):\n",
    "\n",
    "This is **NOT** true MOBO-OSD (Multi-Objective Bayesian Optimization with Orthogonal Search Directions)!\n",
    "\n",
    "**What it is**: Batch Bayesian Optimization with Diversity Penalty (cosine similarity)\n",
    "\n",
    "**Why it's valid**: For pool-based active learning with discrete candidates, this is a practical approximation.\n",
    "\n",
    "**For true MOBO-OSD**: Use BoTorch's `qNoisyExpectedImprovement` with gradient-based orthogonalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell9-code"
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "class BatchDiversityOptimizer:\n",
    "    \"\"\"\n",
    "    Batch Bayesian Optimization with Diversity Penalty.\n",
    "    \n",
    "    This is a pool-based approximation, NOT true MOBO-OSD.\n",
    "    For true orthogonal sampling with gradient projection, see BoTorch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_dict, initial_scores, beta=2.0):\n",
    "        self.embeddings_dict = embeddings_dict\n",
    "        self.all_ids = list(embeddings_dict.keys())\n",
    "        \n",
    "        self.labeled_ids = list(initial_scores.keys())\n",
    "        self.unlabeled_ids = [sid for sid in self.all_ids if sid not in initial_scores]\n",
    "        \n",
    "        self.X_train = np.array([embeddings_dict[sid] for sid in self.labeled_ids])\n",
    "        self.y_train = np.array([initial_scores[sid] for sid in self.labeled_ids])\n",
    "        \n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        \n",
    "        self.X_train_scaled = self.scaler_X.fit_transform(self.X_train)\n",
    "        self.y_train_scaled = self.scaler_y.fit_transform(self.y_train.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        kernel = ConstantKernel(1.0) * Matern(nu=2.5, length_scale=1.0)\n",
    "        self.gp = GaussianProcessRegressor(\n",
    "            kernel=kernel,\n",
    "            n_restarts_optimizer=10,\n",
    "            alpha=1e-6,\n",
    "            normalize_y=False\n",
    "        )\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.iteration = 0\n",
    "        \n",
    "        print(f\"Initialized with {len(self.labeled_ids)} labeled samples\")\n",
    "        print(f\"Pool size: {len(self.unlabeled_ids)} unlabeled\")\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Train Gaussian Process\"\"\"\n",
    "        self.gp.fit(self.X_train_scaled, self.y_train_scaled)\n",
    "        print(f\"GP trained. Kernel: {self.gp.kernel_}\")\n",
    "    \n",
    "    def acquisition_ucb(self, X_pool_scaled):\n",
    "        \"\"\"Upper Confidence Bound: Œº + Œ≤ * œÉ\"\"\"\n",
    "        mu, sigma = self.gp.predict(X_pool_scaled, return_std=True)\n",
    "        return mu + self.beta * sigma\n",
    "    \n",
    "    def select_batch_diverse(self, batch_size=10, diversity_weight=0.5):\n",
    "        \"\"\"\n",
    "        Greedy batch selection with cosine similarity penalty.\n",
    "        \"\"\"\n",
    "        if len(self.unlabeled_ids) == 0:\n",
    "            print(\"‚ö†Ô∏è No unlabeled samples!\")\n",
    "            return []\n",
    "        \n",
    "        X_pool = np.array([self.embeddings_dict[sid] for sid in self.unlabeled_ids])\n",
    "        X_pool_scaled = self.scaler_X.transform(X_pool)\n",
    "        \n",
    "        acq_scores = self.acquisition_ucb(X_pool_scaled)\n",
    "        \n",
    "        selected_indices = []\n",
    "        selected_embeddings = []\n",
    "        \n",
    "        for step in range(min(batch_size, len(self.unlabeled_ids))):\n",
    "            if step == 0:\n",
    "                best_idx = np.argmax(acq_scores)\n",
    "            else:\n",
    "                selected_matrix = np.array(selected_embeddings)\n",
    "                pool_matrix = X_pool\n",
    "                \n",
    "                # Normalize for cosine similarity\n",
    "                selected_norm = selected_matrix / (np.linalg.norm(selected_matrix, axis=1, keepdims=True) + 1e-8)\n",
    "                pool_norm = pool_matrix / (np.linalg.norm(pool_matrix, axis=1, keepdims=True) + 1e-8)\n",
    "                \n",
    "                similarities = np.dot(pool_norm, selected_norm.T)\n",
    "                max_similarity = np.abs(similarities).max(axis=1)\n",
    "                \n",
    "                diversity_penalty = max_similarity * diversity_weight\n",
    "                adjusted_scores = acq_scores * (1 - diversity_penalty)\n",
    "                adjusted_scores[selected_indices] = -np.inf\n",
    "                \n",
    "                best_idx = np.argmax(adjusted_scores)\n",
    "            \n",
    "            selected_indices.append(best_idx)\n",
    "            selected_embeddings.append(X_pool[best_idx])\n",
    "            acq_scores[best_idx] = -np.inf\n",
    "        \n",
    "        selected_ids = [self.unlabeled_ids[i] for i in selected_indices]\n",
    "        \n",
    "        print(f\"\\nSelected {len(selected_ids)} candidates:\")\n",
    "        for i, sid in enumerate(selected_ids):\n",
    "            print(f\"  {i+1}. {sid}\")\n",
    "        \n",
    "        return selected_ids\n",
    "    \n",
    "    def update(self, new_scores):\n",
    "        \"\"\"Update with new experimental results\"\"\"\n",
    "        for sid, score in new_scores.items():\n",
    "            if sid in self.unlabeled_ids:\n",
    "                self.labeled_ids.append(sid)\n",
    "                self.unlabeled_ids.remove(sid)\n",
    "        \n",
    "        self.X_train = np.array([self.embeddings_dict[sid] for sid in self.labeled_ids])\n",
    "        self.y_train = np.array([new_scores.get(sid, self.y_train[i]) \n",
    "                                 for i, sid in enumerate(self.labeled_ids)])\n",
    "        \n",
    "        self.X_train_scaled = self.scaler_X.fit_transform(self.X_train)\n",
    "        self.y_train_scaled = self.scaler_y.fit_transform(self.y_train.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        self.iteration += 1\n",
    "        print(f\"‚úÖ Updated. Iteration {self.iteration}, Labeled: {len(self.labeled_ids)}\")\n",
    "\n",
    "print(\"‚úÖ BatchDiversityOptimizer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell10-header"
   },
   "source": [
    "## üîÑ Cell 10: Run Active Learning Loop (Demo)\n",
    "\n",
    "### ‚ö†Ô∏è MODIFICATION REQUIRED:\n",
    "\n",
    "Replace simulated data with your **actual initial experiments**:\n",
    "\n",
    "```python\n",
    "initial_scores = {\n",
    "    'CAR_001': 0.85,  # Your actual measured scores\n",
    "    'CAR_023': 1.23,\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell10-code"
   },
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings = np.load(EMBEDDING_PATH, allow_pickle=True).item()\n",
    "print(f\"Loaded {len(embeddings)} embeddings\")\n",
    "\n",
    "# ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è REPLACE WITH YOUR ACTUAL DATA ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "# Example: Simulate 20 initial experiments\n",
    "np.random.seed(42)\n",
    "all_sample_ids = list(embeddings.keys())\n",
    "initial_sample_ids = np.random.choice(all_sample_ids, size=20, replace=False).tolist()\n",
    "initial_scores = {sid: np.random.randn() for sid in initial_sample_ids}\n",
    "\n",
    "print(f\"\\nInitial training set: {len(initial_scores)} samples\")\n",
    "print(f\"Score range: [{min(initial_scores.values()):.2f}, {max(initial_scores.values()):.2f}]\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = BatchDiversityOptimizer(\n",
    "    embeddings_dict=embeddings,\n",
    "    initial_scores=initial_scores,\n",
    "    beta=2.0\n",
    ")\n",
    "\n",
    "# Fit GP\n",
    "optimizer.fit()\n",
    "\n",
    "# Select next batch\n",
    "BATCH_SIZE = 10\n",
    "next_batch = optimizer.select_batch_diverse(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    diversity_weight=0.5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Recommended candidates for next experiments:\")\n",
    "print(\"=\" * 60)\n",
    "for i, sid in enumerate(next_batch):\n",
    "    print(f\"{i+1:2d}. {sid}\")\n",
    "\n",
    "# Save\n",
    "with open(\"/content/selected_batch_round1.txt\", \"w\") as f:\n",
    "    for sid in next_batch:\n",
    "        f.write(f\"{sid}\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Saved to /content/selected_batch_round1.txt\")\n",
    "print(\"\\nüìù Next: Perform manual validation on these candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell11-header"
   },
   "source": [
    "## üîÑ Cell 11: Update Model (After Manual Validation)\n",
    "\n",
    "Run this after completing experiments on the selected batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell11-code"
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Replace with your actual experimental results\n",
    "new_experimental_results = {\n",
    "    next_batch[0]: 1.5,  # Replace with real scores\n",
    "    next_batch[1]: 0.8,\n",
    "    next_batch[2]: -0.3,\n",
    "    # ... add all tested samples\n",
    "}\n",
    "\n",
    "print(\"üìä New results:\")\n",
    "for sid, score in new_experimental_results.items():\n",
    "    print(f\"  {sid}: {score:.3f}\")\n",
    "\n",
    "# Update optimizer\n",
    "optimizer.update(new_experimental_results)\n",
    "optimizer.fit()\n",
    "\n",
    "# Select next batch\n",
    "next_batch_round2 = optimizer.select_batch_diverse(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    diversity_weight=0.5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Round 2 - Recommended candidates:\")\n",
    "print(\"=\" * 60)\n",
    "for i, sid in enumerate(next_batch_round2):\n",
    "    print(f\"{i+1:2d}. {sid}\")\n",
    "\n",
    "# Continue this loop until convergence..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell12-header"
   },
   "source": [
    "## üìä Cell 12: Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell12-code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Prepare data\n",
    "labeled_embeddings = np.array([embeddings[sid] for sid in optimizer.labeled_ids])\n",
    "labeled_scores = optimizer.y_train\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "labeled_2d = pca.fit_transform(labeled_embeddings)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Colored by score\n",
    "scatter = axes[0].scatter(\n",
    "    labeled_2d[:, 0], labeled_2d[:, 1],\n",
    "    c=labeled_scores, cmap='viridis',\n",
    "    s=100, alpha=0.6, edgecolors='black'\n",
    ")\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].set_title('Embedding Space (Colored by Score)')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Score')\n",
    "\n",
    "# Mark selected\n",
    "if 'next_batch' in locals():\n",
    "    next_batch_embeddings = np.array([embeddings[sid] for sid in next_batch])\n",
    "    next_batch_2d = pca.transform(next_batch_embeddings)\n",
    "    axes[0].scatter(\n",
    "        next_batch_2d[:, 0], next_batch_2d[:, 1],\n",
    "        c='red', s=200, alpha=0.8, marker='*',\n",
    "        edgecolors='black', linewidths=2,\n",
    "        label='Selected'\n",
    "    )\n",
    "    axes[0].legend()\n",
    "\n",
    "# Plot 2: Coverage\n",
    "axes[1].scatter(labeled_2d[:, 0], labeled_2d[:, 1],\n",
    "                c='blue', s=50, alpha=0.3, label='Labeled')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].set_title('Search Space Coverage')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/visualization.png', dpi=150, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved to /content/visualization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell13-header"
   },
   "source": [
    "## üíæ Cell 13: Save Results & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell13-code"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = f\"/content/drive/My Drive/BioFoundry/results_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save embeddings\n",
    "np.save(f\"{results_dir}/embeddings.npy\", embeddings)\n",
    "\n",
    "# Save optimizer state\n",
    "with open(f\"{results_dir}/optimizer_state.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        'labeled_ids': optimizer.labeled_ids,\n",
    "        'unlabeled_ids': optimizer.unlabeled_ids,\n",
    "        'scores': dict(zip(optimizer.labeled_ids, optimizer.y_train)),\n",
    "        'iteration': optimizer.iteration\n",
    "    }, f)\n",
    "\n",
    "# Save batches\n",
    "with open(f\"{results_dir}/selected_batches.txt\", \"w\") as f:\n",
    "    f.write(f\"# Active Learning Results - {timestamp}\\n\\n\")\n",
    "    f.write(\"## Round 1:\\n\")\n",
    "    for sid in next_batch:\n",
    "        f.write(f\"{sid}\\n\")\n",
    "\n",
    "# Copy checkpoint\n",
    "shutil.copy(checkpoint_path, f\"{results_dir}/best_model.pt\")\n",
    "\n",
    "print(f\"‚úÖ Results saved to {results_dir}\")\n",
    "print(\"\\nüìÇ Files:\")\n",
    "print(\"  - embeddings.npy\")\n",
    "print(\"  - optimizer_state.pkl\")\n",
    "print(\"  - selected_batches.txt\")\n",
    "print(\"  - best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell14-header"
   },
   "source": [
    "## ‚úÖ Cell 14: Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ‚úÖ Trained EquiformerV2 on CAR-T dataset\n",
    "2. ‚úÖ Extracted geometric embeddings (corrected Hook method)\n",
    "3. ‚úÖ Implemented Batch Diversity Sampling\n",
    "4. ‚úÖ Selected candidates for manual validation\n",
    "\n",
    "### Key Corrections Applied:\n",
    "- ‚úÖ Embedding via `register_forward_hook` (not model output)\n",
    "- ‚úÖ Renamed to Batch Diversity Sampling (not MOBO-OSD)\n",
    "- ‚úÖ GPU-adaptive config (T4: batch=4, lmax=[2])\n",
    "- ‚úÖ LMDB copied to local disk\n",
    "- ‚úÖ scipy==1.13.1 for sph_harm\n",
    "\n",
    "### Next Steps:\n",
    "1. **Manual Validation**: Test selected candidates\n",
    "2. **Record Results**: Measure performance\n",
    "3. **Update Model**: Run Cell 11 with new data\n",
    "4. **Iterate**: Repeat until Pareto frontier convergence\n",
    "\n",
    "### Troubleshooting:\n",
    "- **OOM**: Reduce `batch_size`, `lmax_list`\n",
    "- **Slow**: Check LMDB is on local disk\n",
    "- **Hook fails**: Print model structure, update layer name\n",
    "\n",
    "---\n",
    "\n",
    "**üìà Good luck with your experiments!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell14-code"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üéâ BioFoundry Active Learning Pipeline Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nEmbeddings: {len(embeddings)} samples\")\n",
    "print(f\"Labeled pool: {len(optimizer.labeled_ids)}\")\n",
    "print(f\"Unlabeled: {len(optimizer.unlabeled_ids)}\")\n",
    "print(f\"\\nResults: {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BioFoundry_ActiveLearning_Colab.ipynb",
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
